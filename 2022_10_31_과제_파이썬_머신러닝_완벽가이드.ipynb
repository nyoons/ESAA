{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNg3dCwPCamtpb+l90e5hnh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyoons/ESAA/blob/main/2022_10_31_%EA%B3%BC%EC%A0%9C_%ED%8C%8C%EC%9D%B4%EC%8D%AC_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%99%84%EB%B2%BD%EA%B0%80%EC%9D%B4%EB%93%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ch 03. 평가\n",
        "머신러닝의 프로세스는 데이터 가공-모델 학습과 예측-평가. 성능평가지표는 모델이 분류인지 회귀인지에 따라 여러 종류로 나뉜다. 특히 분류의 성능평가지표로는 정확도, 오차행렬, 정밀도, 재현율, F1 스코어, ROC AUC 등이 있다. 이진/멀티분류에 모두 적용할 수 있지만 이진분류에서 더욱 강조하는 지표.\n",
        ">**01.정확도**\n",
        "\n",
        "=예측 결과가 동일한 데이터 건수 / 전체 예측 데이터 건수\n",
        "<br/>데이터의 구성에 따라 모델의 성능을 왜곡할 수 있어서 수치 하나로는 평가하지 않음. 어떻게 왜곡?\n",
        "<br/>앞의 타이타닉 고려. ml알고리즘 적용 후 예측 정확도는 보통 80퍼대였지만, 탑승객이 남자인 경우보다 여자인 경우 생존 확률이 높았으니 별다른 알고리즘 없이 무조건 여자인 경우 생존, 남자인 경우 사망으로 예측해도 비슷한 수치가 나올수도. 성별 조건 하나만으로 결정하는 별거 아닌 알고리즘도 정확도가 높아짐.\n",
        "<br/>BaseEstimator 받아 아무런 학습 없이 성별에 따라 예측하는 단순 classifier 생성해보자"
      ],
      "metadata": {
        "id": "BImAh1ilKK0X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a35iD1gvJe_T"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class MyDummyClassifier(BaseEstimator):\n",
        "  #fit메서드는 아무것도 학습하지 않게\n",
        "  def fit(self, X, y=None):\n",
        "    pass\n",
        "  #predict() 메서드는 단순히 sex 피처가 1이면 0, 아니면 1로 예측\n",
        "  def predict(self, X):\n",
        "    pred=np.zeros((X.shape[0],1))\n",
        "    for i in range(X.shape[0]) :\n",
        "      if X['Sex'].iloc[i]==1:\n",
        "        pred[i]=0\n",
        "      else :\n",
        "        pred[i]=1\n",
        "\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "생성된 MyDummyClassifier 이용해 타이타닉 생존자 예측 해보자"
      ],
      "metadata": {
        "id": "GImOoEtCM3fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#원본 데이터 재로딩, 데이터 가공, 학습 데이터/테스트 데이터로 분할\n",
        "titanic_df=pd.read_csv('titanic_train.csv')\n",
        "y_titanic_df=titanic_df['Survived']\n",
        "X_titanic_df=titanic_df.drop('Survived', axis=1)\n",
        "X_titanic_df=transform_features(X_titanic_df)\n",
        "X_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df, test_size=0.2, random_state=0)\n",
        "\n",
        "#위에서 생선한 Dummy Classifier 이용해 학습/예측/평가 수행\n",
        "myclf=MyDummyClassifier()\n",
        "myclf.fit(X_train, y_train)\n",
        "\n",
        "mypredictions=myclf.predict(X_test)\n",
        "print('Dummy Classfier의 정확도는: {0:.4f}'.format(accuracy_score(y_test, mypredictions)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "tsZYB-gUNBQ_",
        "outputId": "14433882-7341-4af8-90dc-0a5494d8c2a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-56594c500ed8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my_titanic_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtitanic_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mX_titanic_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtitanic_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_titanic_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_titanic_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_titanic_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_titanic_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transform_features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이렇게만 해도 정확도가 0.7877. 정확도는 불균형한 레이블 값 분포에서 적합하지 않음.\n",
        "<br/>MNIST로도 보자. 7만 True, 아닌건 False로 변환해 이진 분류로 바꿔 예측하자."
      ],
      "metadata": {
        "id": "TDn7LBudO2z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class MyDummyClassifier(BaseEstimator):\n",
        "  def fit(self, X, y):\n",
        "    pass\n",
        "\n",
        "  #입력값으로 들어오는 X 데이터 세트 크기만큼 모두 0값으로 만들어 반환\n",
        "  def predict(self, X):\n",
        "      return np.zeros( (len(X), 1), dtype=bool)\n",
        "\n",
        "#사이킷런의 내장 데이터 세트인 load_digits 이용해 MNIST 데이터 로딩\n",
        "digits=load_digits()\n",
        "\n",
        "#digits 번호가 7이면 TRUE 이고 이를 astype(int)로 1로 변환, 7 아니면 False 이고 0으로 변환.\n",
        "y=(digits.target==7).astype(int)\n",
        "X_train, X_test, y_train, y_test=train_test_split(digits.data, y, random_state=11)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "oYH9EiioPVg5",
        "outputId": "48a80016-1745-402d-ec01-c26adb869d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-46de964cacee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#사이킷런의 내장 데이터 세트인 load_digits 이용해 MNIST 데이터 로딩\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_digits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#digits 번호가 7이면 TRUE 이고 이를 astype(int)로 1로 변환, 7 아니면 False 이고 0으로 변환.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_digits' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "불균형한 데이터로 생성한 y_test의 분포 확인, 예측/평가 수행"
      ],
      "metadata": {
        "id": "R1yHLmSaBMsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#불균형한 레이블 데이터 분포 확인\n",
        "print('레이블 테스트 세트 크기 :', y_test.shape)\n",
        "print('테스트 세트 레이블 0과 1의 분포도 : ')\n",
        "print(pd.Series(y_test).value_counts())\n",
        "\n",
        "#Dummy Classifier로 학습/예측 정확도 평가\n",
        "fakeclf=MyFakeClassifier()\n",
        "fakeclf.fit(X_train, y_train)\n",
        "fakepred=fakeclf.predict(X_test)\n",
        "print('모든 예측을 0으로 하여도 정확도는:{:.3f}'.format(accuracu_score(y_test,fakepred)))"
      ],
      "metadata": {
        "id": "52PcQnacBRqo",
        "outputId": "1ed6346d-408f-43b5-a6f7-0aa9e6731da8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-13cb03114c09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#불균형한 레이블 데이터 분포 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'레이블 테스트 세트 크기 :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'테스트 세트 레이블 0과 1의 분포도 : '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이렇게만 해도 정확도가 90퍼. 정확도는 불균형한 레이블 데이터 세트에서는 사용하면 안됨."
      ],
      "metadata": {
        "id": "xCIol69uB9sL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">**2. 오차 행렬**\n",
        "\n",
        "학습된 분류 모델이 예측을 수행하며 얼마나 헷갈리고 있는지 보여주는 지표. TN, FP, FN, TP를 생각하자.\n",
        "<br/>사이킷런은 오차 행렬을 구하기 위해 confusion_matrix() 제공"
      ],
      "metadata": {
        "id": "IK2Vh6-kCFuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, fakepred)\n"
      ],
      "metadata": {
        "id": "HlVbbVczCz_k",
        "outputId": "7b043117-c708-48c1-8910-ab310b5caf6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0418b531113e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfakepred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ndarray 형태로 출력. TP, TN, FN, FP를 보면 정확도가 왜 오류를 일으키는지 알 수 있다."
      ],
      "metadata": {
        "id": "5zURwb1LC9Lj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **3. 정밀도와 재현율**\n",
        "\n",
        "positive 데이터 세트의 예측 성능에 초점을 맞춘 평가 지표. 앞에서 만든 myfakeclf는 positive로 예측한 TP 값이 하나도 없어서 정밀도와 재현율이 모두 0.\n",
        "<br/>정밀도=TP/(FP+TP)\n",
        "<br/>재현율=TP/(FN+TP) = 민감도, TPR\n",
        "<br/>재현율이 중요 지표인 경우는 실제 positive 양성 데이터를 negative로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우. 보험사기나 금융사기 적발 모델, 암 판단 모델 등.\n",
        "<br/>정밀도가 중요한 지표인 경우는 스패메일 여부를 판단하는 모델. 실제 negative 음성인 데이터 예측을 positive 양성으로 잘못 판단하면 업무상 큰 영향이 발생하는 경우.\n",
        "<br/>사이킷런은 정밀도 계산을 위해 precision_score(), 재현율 계산을 위해 recall_score() 제공"
      ],
      "metadata": {
        "id": "_eyfDuUpDaky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "def get_clf_eval(y_test, pred):\n",
        "  confusion=confusion_matrix(y_test, pred)\n",
        "  accuracy=precision_score(y_test, pred)\n",
        "  precision=precision_score(y_test, pred)\n",
        "  recall=recall_score(y_test, pred)\n",
        "  print('오차 행렬')\n",
        "  print(confusion)\n",
        "  print('정확도:{0:.4f}, 재현율:{2:.4f}'.format(accuracy, precision, recall))\n"
      ],
      "metadata": {
        "id": "LA8y-OX-EnDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#이제 로지스틱 회귀 기반으로 타이타닉 예측하고 평가 수행.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#원본 데이터 재로딩, 데이터 가공, 학습 데이터/테스트 데이터 분할\n",
        "titanic_df=pd.read_csv('titanic_train.csv')\n",
        "y_titanic_df=titanic_df['Survived']\n",
        "X_titanic_df=titanic_df.drop('Survived', axis=1)\n",
        "X_titanic_df=transform_features(X_titanic_df)\n",
        "\n",
        "X_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df, test_size=0.20, random_state=11)\n",
        "\n",
        "lr_clf=LogisticRegression()\n",
        "lr_clf.fit(X_train, y_train)\n",
        "pred=lr_clf.predict(X_test)\n",
        "get_clf_eval(y_test,pred)"
      ],
      "metadata": {
        "id": "6STgLM18FW0u",
        "outputId": "3cf1018b-b4b6-47f0-819f-14e7578fc334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-77feb110281e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#원본 데이터 재로딩, 데이터 가공, 학습 데이터/테스트 데이터 분할\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtitanic_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'titanic_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my_titanic_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtitanic_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mX_titanic_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtitanic_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic_train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "재현율이 좀 낮게 나왔다. 강화할 방법?\n",
        "\n",
        "###정밀도/재현율 트레이드오프\n",
        "\n",
        "업무 특성에 따라 정밀도나 재현율이 강조돼야 할 경우 분류의 결정 임계값을 조정해 수치를 높일 수 있다. 하지만 둘은 상호 보완적.\n",
        "<br/>사이킷런의 분류 알고리즘은 예측 데이터가 특정 레이블에 속하는지 계산하기 위해먼저 개별 레이블별 결정 확률을 구하고, 큰 예측 확률로 예측. 이진 분류에서 임계값은 0.5. predict_proba() 제공. \n"
      ],
      "metadata": {
        "id": "Mz-CMBJJFVkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_proba=lr_clf.predict_proba(X_test)\n",
        "pred=lr_clf.predict(X_test)\n",
        "print('pred_proba()결과 Shape:{0}'.format(pred_proba.shape))\n",
        "print('pred_proba array에서 앞 3개만 샘플로 추출 \\n:', pred_proba[:3])\n",
        "\n",
        "#예측 확률 array와 예측 결과값 array를 병합해 예측 확률과 결과값을 한눈에 확인\n",
        "pred_proba_result=np.concatenate([pred_proba, pred.reshape(-1,1)], axis=1)\n",
        "print('두 개의 class 중에서 더 큰 확률을 클래스 값으로 예측 \\n', pred_proba_result[:3])"
      ],
      "metadata": {
        "id": "wFhY47SIZA2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "반환 결과의 ndarray는 0과 1에 대한 확률. 더 큰 값으로 예측.\n",
        "<br/>predict 매서드가 predict_proba 메서드에 기반된 것. 이제 트레이드오프 이해해보자.\n",
        "<br/>사이킷런의 Binarizer 클래스를 이용. 임계값 변수를 특정 값으로 설정하고 binarizer 클래스를 객체로 생성해보자."
      ],
      "metadata": {
        "id": "SR7Kh6JdZqfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Binarizer\n",
        "X=[[1,-1,2],\n",
        "   [2,0,0],\n",
        "   [0,1.1,1.2]]\n",
        "#X의 개별 원소들이 임계값보다 작거나 같으면 0, 아니면 1 반환\n",
        "binarizer=Binarizer(threshold=1.1)\n",
        "print(binarizer.fit_transform(X))"
      ],
      "metadata": {
        "id": "w21cpZXMaG7x",
        "outputId": "8319bdeb-66ff-4020-f134-adeeae2276a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 이 binarizer 이용해 predict()의 의사 코드를 만들어보자"
      ],
      "metadata": {
        "id": "YTcuJrzBahqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Binarizer\n",
        "#Binarizer이 임계값 설정값. 분류 결정 임계값임.\n",
        "custom_threshold=0.5\n",
        "\n",
        "#predict_proba 반환값의 두번째 칼럼, 즉 positive 클래스 칼럼 하나만 추출해 적용\n",
        "pred_proba_1=pred_proba[:,1].reshape(-1,1)\n",
        "\n",
        "binarizer=Binarizer(threshold=custom_threshold).fit(pred_proba_1)\n",
        "custom_predict=binarizer.transform(pred_proba_1)\n",
        "\n",
        "get_clf_eval(y_test, custom_predict)\n"
      ],
      "metadata": {
        "id": "jCWq-nJyaoPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "오차 행렬 도출됨. 임계값을 낮추면?"
      ],
      "metadata": {
        "id": "8XbxsvjnbPsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_threshold=0.4\n",
        "\n",
        "#predict_proba 반환값의 두번째 칼럼, 즉 positive 클래스 칼럼 하나만 추출해 적용\n",
        "pred_proba_1=pred_proba[:,1].reshape(-1,1)\n",
        "\n",
        "binarizer=Binarizer(threshold=custom_threshold).fit(pred_proba_1)\n",
        "custom_predict=binarizer.transform(pred_proba_1)\n",
        "\n",
        "get_clf_eval(y_test, custom_predict)\n"
      ],
      "metadata": {
        "id": "ELlMSpqEbUgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "재현율 올라가고 정밀도 감소. 분류 결저 임계값은 positive 예측값을 결정하는 확률의 기준이 된다. 예측을 너그럽게 하게 되면 True 값이 많아진다. 그렇다면 재현율이 높아지지.\n",
        "<br/>임계값을 0.4에서 0.6까지 0.05씩 증가시키며 지표 조사하자."
      ],
      "metadata": {
        "id": "48hedB9bbaso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트를 수행할 모든 임계값을 리스트 객체로 저장.\n",
        "thresholds=[0.4,0.45,0.5,0.55,0.6]\n",
        "\n",
        "def get_eval_by_threshold(y_test,pred_proba_c1,thresholds):\n",
        "  #thresholds list 객체 내의 값 차례로 반복하며 평가 수행\n",
        "  for custom_threshold in thresholds:\n",
        "    binarizer=Binarizer(threshold=custom_threshold).fit(pred_proba_c1)\n",
        "    custom_predict=binarizer.transform(pred_proba_c1)\n",
        "    print('임계값:', custom_threshold)\n",
        "    get_clf_eval(y_test, custom_predict)\n",
        "\n",
        "get_eval_by_threshold(y_test, pred_proba[:1].reshape(-1,1), thresholds)"
      ],
      "metadata": {
        "id": "OjgzaZembwC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.45일 경우 0.5에 비해 정밀도는 약간 떨어지나 재현율이 오름. 정확도는 동일. 재현율을 향상시키려면 0.45가 적당.\n",
        "<br/>이렇게 임계값 변화에 따른 평가 지표 값을 알아보려면 사이킷런의 precision_recall_curve() 사용.\n",
        "<br/>타이타닐 적용해보자. 인자로 실제 값 데이터 세트와 레이블 값이 1일 때의 예측 확률을 입력."
      ],
      "metadata": {
        "id": "c_YmKOF5cmvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "#레이블값이 1인 경우 예측 확률 추출\n",
        "pred_proba_class1=lr_clf.predict_proba(X_test)[:,1]\n",
        "\n",
        "#실제 값 데이터 세트와 레이블 값이 1일 때의 예측 확률을 precision_recall_curve 인자로 입력\n",
        "precisions, recalls, thresholds=precision_recall_curve(y_test,pred_proba_class1)\n",
        "print('반환된 분류 결정 임계값 배열의 shape:', thresholds.shape)\n",
        "\n",
        "#반환된 임계값 배열 로우가 147건이므로 샘플로 10건만 추출, 임계값을 15step으로 추출.\n",
        "thr_idx=np.arange(0, thresholds.shape[0], 15)\n",
        "print('샘플 추출을 위한 임계값 배열의 index 10개:', thr_idx)\n",
        "print('샘플용 10개의 임계값:', np.round(thresholds[thr_idx],2))\n",
        "\n",
        "#15step 단위로 추출된 임계값에 따른 정밀도와 재현율 값\n",
        "print('샘플 임계값 별 정밀도:', np.round(precisions[thr_idx],3))\n",
        "print('샘플 임계값 별 재현율:', np.round(recalls[thr_idx], 3))"
      ],
      "metadata": {
        "id": "nLIx35obdKC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "임계값 증가할수록 정밀도 높아지나 재현율은 낮아짐. precison_recall_curve() 사용해 재현율의 변화를 곡선으로 시각화하자."
      ],
      "metadata": {
        "id": "RspyE7EUeQ_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.errors import InvalidIndexError\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "def precision_recall_curve_plot(y_test, pred_proba_c1):\n",
        "  #threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출.\n",
        "  precisions, recalls, thresholds = precison_recall_curve(y_test, pred_proba_c1)\n",
        "\n",
        "  #X출을 threshold 값으로, y축은 정밀도, 재현율 값으로 각각 plot 수행. 정밀도는 점선으로 표시\n",
        "  plt.figure(figsize=(8,6))\n",
        "  threshold_boundary=thresholds.shape[0]\n",
        "  plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='-', label='precision')\n",
        "  plt.plot(thresholds, recalls[0:threshold_boundary], label='recall')\n",
        "\n",
        "  #thresholds 값 x축의 scale을 0.1 단위로 변경\n",
        "  start, end=plt.xlim()\n",
        "  plt.xticks(np.round(np.arange(start, end, 0.1),2))\n",
        "\n",
        "  #x,y축 label과 legend, grid 설정\n",
        "  plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall curve')\n",
        "  plt.legend(); plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "precision_recall_curve_plot(y_test, lr_clf.predict_proba(X_test)[:,1])"
      ],
      "metadata": {
        "id": "1s53FAixedV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "역시 임계값이 낮을수록 재현율이 높고 정밀도가 낮다. "
      ],
      "metadata": {
        "id": "CD0K2Qn5gtNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###정밀도와 재현율의 맹점\n",
        "상호 보완적.\n",
        "\n",
        "#### 정밀도가 100%가 되는 방법\n",
        "확실한 기준이 되는 경우만 positive, 나머지는 모두 negative로 예측\n",
        "#### 재현율이 100%가 되는 방법\n",
        "모든 환자를 positive로 예측.\n",
        "\n",
        "이처럼 극단적 수치 조작 가능... 그러나 균형 있게 사용해야함\n",
        "\n",
        "<br/>\n",
        "\n",
        ">**4. F1 스코어**\n",
        "\n",
        "정밀도와 재현율을 결합한 지표. 어느 한쪽으로 치우치지 않을 때 높은 수치. 사이킷런의 f1_score() 이용."
      ],
      "metadata": {
        "id": "GTHJEobbg0eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1=f1_score(y_test, pred)\n",
        "print('F1 스코어: {0:.4f}'.format(f1))"
      ],
      "metadata": {
        "id": "7Pd1MQmchfYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "타이타닉에서 임계값 변화시키며 f1 스코어 등 평가 지표 구해보자."
      ],
      "metadata": {
        "id": "uDevuvNUh_M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clf_eval(y_test, pred):\n",
        "  confusion=confusion_matrix(y_test, pred)\n",
        "  accuracy=accuracy_score(y_test, pred)\n",
        "  precision=precision_score(y_test, pred)\n",
        "  recall=recall_score(y_test, pred)\n",
        "  #F1 스코어 추가\n",
        "  f1=f1_score(y_test, pred)\n",
        "  print('오차 행렬')\n",
        "  print(confusion)\n",
        "  #f1 score print 추가\n",
        "  print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1:{3:.4f}'.format(accuracy, precision, recall, f1))\n",
        "\n",
        "thresholds=[0.4,0.45,0.5,0.55,0.6]\n",
        "pred_proba=lr_clf.predict_proba(X_test)\n",
        "get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1), thresholds)"
      ],
      "metadata": {
        "id": "7R44EekXiFhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "f1 스코어는 임계값이 0.6일때 가장 좋다.\n",
        "\n",
        ">**5. ROC 곡선과 AUC**\n",
        "\n",
        "ROC 곡선은 FPR이 변할 때 TPR이 어떻게 변하는가. \n",
        "<br/>ROC 곡선은 FPR을 0부터 1까지 변화시키며 TPR의 값을 구한다. 임계값을 변경하며 수행하면 된다. 임계값=1이면 FPR=0, 임계값=0이면 FPR=1"
      ],
      "metadata": {
        "id": "B0Zc8C9zjC8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "#레이블 값이 1일 때의 예측 확률 추출\n",
        "pred_proba_class1=lr_clf.predict_proba(X_test)[:,1]\n",
        "\n",
        "fprs, tprs, thresholds=roc_curve(y_test, pred_proba_class1)\n",
        "#반환된 임계값 배열 로우가 47건이므로 샘플로 10건만 추출, 임계값을 5step으로 추출.\n",
        "#thresholds[0]은 max(예측확률)+1로 임의 설정. 제외하기 위해 np.arange는 1부터 시작.\n",
        "thr_index=np.arange(1,thresholds.shape[0], 5)\n",
        "\n",
        "print('샘플 추출을 위한 임계값 배열의 index 10개:', thr_index)\n",
        "print('샘플용 10개의 임계값:', np.round(thresholds[thr_index],2))\n",
        "\n",
        "#5step 단위로 추출된 임계값에 따른 FPR, TPR 값\n",
        "print('샘플 임계값 별 FPR:', np.round(fprs[thr_index], 3))\n",
        "print('샘플 임계값 별 TPR:', np.round(tprs[thr_index], 3))"
      ],
      "metadata": {
        "id": "1A0W-202j3Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "임계값이 1이 가까운 값에서 점점 작아지며 FPR이 커진다. 그러면서 TPR은 가파르게 증가. 시각화하면 더 잘 보인다!\n",
        "<br/>ROC 곡선 자체는 FPR과 TPR의 변화 값을 보는데 이용하고 성능 지표로 사용되는건 AUC (면적)."
      ],
      "metadata": {
        "id": "Hk172Xcyk9kQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
        "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "print(confusion_matrix(y_target, preds))\n",
        "print('정확도:', np.round(accuracy_score(y_target, preds), 4))\n",
        "print('정밀도:', np.round(precision_score(y_target, preds), 4))\n",
        "print('재현율:', np.round(recall_score(y_target, preds), 4))"
      ],
      "metadata": {
        "id": "DOiK0wTKlZ68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUC 값은 약 0.8987. 마지막으로 get_clf_eval에 roc_auc_score 이용해 추가하자."
      ],
      "metadata": {
        "id": "wfEsC7XvmC9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
        "  confusion=confusion_matrix(y_test, pred)\n",
        "  accuracy=accuracy_score(y_test, pred)\n",
        "  precision=precision_score(y_test, pred)\n",
        "  recall=recall_score(y_test, pred)\n",
        "  f1=f1_score(y_test, pred)\n",
        "  roc_auc=roc_auc_score(y_test, pred_proba)\n",
        "  print('오차 행렬')\n",
        "  print(confusion)\n",
        "  print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
        "       F1:{3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))"
      ],
      "metadata": {
        "id": "PQqPNFzCmRK1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}